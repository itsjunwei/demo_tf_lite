{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf \n",
    "import librosa\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting Audio Files\n",
    "\n",
    "Core files are separated into Dog , Speech , Impact\n",
    "\n",
    "Segment into 200ms chunks with 100ms overlap \n",
    "\n",
    "Saved in \"class_azimuth_idx.wav\" file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_types = ['Dog', 'Impact' , 'Speech']\n",
    "for ct in class_types:\n",
    "    \n",
    "    # raw data directory\n",
    "    full_data_dir = '../dataset/data/Dataset_concatenated_tracks/{}/'.format(ct)\n",
    "    # cleaned, output data directory\n",
    "    output_data_dir = '../dataset/cleaned_data/{}'.format(ct.lower())\n",
    "    \n",
    "    # create dirs\n",
    "    os.makedirs(output_data_dir, exist_ok=True)\n",
    "    \n",
    "    # Manual settings \n",
    "    fs=48000\n",
    "    frame_len = int(0.2*fs) # 200ms\n",
    "    hop_len = int(0.1*fs)   # 100ms\n",
    "    \n",
    "    # loop through raw data dir\n",
    "    for file in os.listdir(full_data_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            fullfn = os.path.join(full_data_dir, file)\n",
    "            \n",
    "            # extract azimuth from gt\n",
    "            vars = file.split('_')\n",
    "            azimuth = vars[2]\n",
    "            \n",
    "            # load audio\n",
    "            audio , _ = librosa.load(fullfn, sr=fs, mono=False, dtype=np.float32)\n",
    "            \n",
    "            # convert the dB increase to a linear scale\n",
    "            # dB_increase = 10\n",
    "            # factor = np.power(10.0, dB_increase/20.0)\n",
    "            \n",
    "            # audio *= factor \n",
    "            \n",
    "            # Segment the audio input into overlapping frames\n",
    "            frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop_len)\n",
    "            \n",
    "            # Transpose into (n_segments, timebins, channels)\n",
    "            frames = frames.T\n",
    "            for idx, frame in enumerate(tqdm(frames)):\n",
    "                final_fn = \"{}_{}_{}.wav\".format(ct.lower(), azimuth, idx+1)\n",
    "                final_fp = os.path.join(output_data_dir, final_fn)\n",
    "                sf.write(final_fp, frame, samplerate=48000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking SALSA-Lite feature dimensions/parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmax_doa = 2000\n",
    "n_fft = 1024\n",
    "fmin_doa = 50 \n",
    "fs = 48000\n",
    "fmax_doa = np.min((fmax_doa, fs // 2))\n",
    "n_bins = n_fft // 2 + 1\n",
    "lower_bin = int(np.floor(fmin_doa * n_fft / float(fs)))  # 512: 1; 256: 0\n",
    "upper_bin = int(np.floor(fmax_doa * n_fft / float(fs)))  # 9000Hz: 512: 192, 256: 96\n",
    "lower_bin = np.max((1, lower_bin))\n",
    "\n",
    "print(fmax_doa, fmin_doa, upper_bin, lower_bin)\n",
    "\n",
    "fmax = 9000  # Hz\n",
    "cutoff_bin = int(np.floor(fmax * n_fft / float(fs)))\n",
    "print(cutoff_bin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
