{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing local scaling/normalization functions. \n",
    "\n",
    "`Local scaling` is better if we wish to preserve the dynamic range of the audio data, while `local norming` will be better if we want to preserve the distances between the data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_scaling(x):\n",
    "    \"\"\"Scaling an array to fit between -1 and 1\"\"\"\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    x_normed = (x - x_min) / (x_max - x_min)\n",
    "    x_scaled = 2 * x_normed - 1\n",
    "    \n",
    "    return x_scaled\n",
    "\n",
    "def local_norming(x):\n",
    "    x -= np.mean(x)\n",
    "    x /= np.max(np.abs(x))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to remove silence\n",
    "\n",
    "`remove_silence` is the only working function so far. Alternatives include labelling weak signal points (low absolute value) as noise instead (WIP/KIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence(signal, segment_duration=0.5, threshold=0.001):\n",
    "    \"\"\"Function to brute force remove silent segments of the audio by comparing the absolute normalized\n",
    "    value to a certain threshold. \n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): audio signal\n",
    "        segment_duration (float, optional): Duration of a segment. Essentially the duration of the\n",
    "        periods of empty noise which we are trying to remove. Defaults to 0.5.\n",
    "        threshold (float, optional): Threshold value which we deem to be noise/ambient sound. \n",
    "        Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        new_signal (np.ndarray) : audio signal with the noise removed\n",
    "    \"\"\"\n",
    "    fs = 48000\n",
    "    segment_length = int(segment_duration * fs)\n",
    "    \n",
    "    # Number of segments\n",
    "    num_segments = signal.shape[1] // segment_length\n",
    "\n",
    "    # Initialize a 3D numpy array for the segments\n",
    "    segments = np.zeros((num_segments, signal.shape[0], segment_length))\n",
    "\n",
    "    # Split the signal into segments for each channel\n",
    "    for i in range(num_segments):\n",
    "        segments[i, :, :] = signal[:, i*segment_length:(i+1)*segment_length]\n",
    "\n",
    "    # Calculate the absolute maximum value in each segment for each channel\n",
    "    max_values = np.max(np.abs(segments), axis=2)\n",
    "\n",
    "    # Identify the segments where the maximum value is above the threshold for any channel\n",
    "    keep_segments = np.any(max_values > threshold, axis=1)\n",
    "\n",
    "    # Keep only the segments where the maximum value is above the threshold for any channel\n",
    "    new_signal = np.concatenate(segments[keep_segments, :, :], axis=1)\n",
    "\n",
    "    \n",
    "    return new_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing silence\n",
    "Now we actually cycle through the original dataset directories, for each class, and remove the silence in each concatenated track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing : ./data/Dataset_concatenated_tracks/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing : ./data/Dataset_concatenated_tracks/Impact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing : ./data/Dataset_concatenated_tracks/Speech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:08<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# This should be the originally re-recorded data dir\n",
    "original_audio_dir = \"./data/Dataset_concatenated_tracks/\"\n",
    "\n",
    "# Final audio output dir\n",
    "final_dir = './data/remove_silence/'\n",
    "\n",
    "\"\"\"Active classes and their respective silence duration (how long we deem a silent period to be)\n",
    "The silence duration is technically a hyperparameter we need to find out. Threshold is also \n",
    "a hyperparameter\"\"\"\n",
    "audio_classes = {'Dog' : (0.1, 0.01), 'Impact' : (0.1, 0.01), 'Speech' : (0.1, 0.03)}\n",
    "\n",
    "fs = 48000 # Sample rate, hardcoded\n",
    "for cls , vars in audio_classes.items():\n",
    "    sil_dur = vars[0] # Silent duration\n",
    "    thresh  = vars[1] # Threshold\n",
    "    class_audio_dir = os.path.join(original_audio_dir, cls)\n",
    "    tqdm.write(\"Now processing : {}\".format(class_audio_dir))\n",
    "    for concat_track in tqdm(os.listdir(class_audio_dir)):\n",
    "        if concat_track.endswith('.wav'):\n",
    "            concat_track_fp = os.path.join(class_audio_dir, concat_track)\n",
    "            audio_data , _ = librosa.load(concat_track_fp, sr=fs, mono=False, dtype=np.float32)\n",
    "            \n",
    "            # Here I choose to normalize the audio tracks, so that the mean will be nearer to 0 (but not\n",
    "            # directly 0 if the audio is not balanced around 0)\n",
    "            result = []\n",
    "            for i in range(len(audio_data)):\n",
    "                a = local_norming(audio_data[i]) # Change to normalization\n",
    "                result.append(a)\n",
    "            result = np.array(result)\n",
    "\n",
    "            # Remove the silence from the audio\n",
    "            result_remove_silence = remove_silence(result, \n",
    "                                                   segment_duration=sil_dur,\n",
    "                                                   threshold=thresh)\n",
    "            \n",
    "            # Write them to output file\n",
    "            os.makedirs(os.path.join(final_dir, cls), exist_ok=True)\n",
    "            no_silence_file_dir = os.path.join(final_dir, cls, concat_track)\n",
    "            sf.write(no_silence_file_dir, result_remove_silence.T, samplerate=fs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-lite-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
