{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from augmented_dataset_generator import full_feature_with_norm\n",
    "import yaml\n",
    "import soundfile as sf\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions to create features, and choose which frames to replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_scaling(x):\n",
    "    \"\"\"Scaling an array to fit between -1 and 1\"\"\"\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    x_normed = (x - x_min) / (x_max - x_min)\n",
    "    x_scaled = 2 * x_normed - 1\n",
    "    \n",
    "    return x_scaled\n",
    "\n",
    "def extract_features(audio_data,\n",
    "                     cfg = None,\n",
    "                     data_config: str = './configs/salsa_lite_demo_3class.yml'\n",
    "                    ) -> None:\n",
    "    \"\"\"\n",
    "    Extract SALSA-Lite features from a segment of audio \n",
    "    SALSA-Lite consists of:\n",
    "        - 4 Log-linear spectrograms \n",
    "        - 3 Normalized Interchannel Phase Differences \n",
    "\n",
    "    [This needs to be revised for the demo]\n",
    "    The frequency range of log-linear spectrogram is 0 to 9kHz.\n",
    "\n",
    "    Arguments\n",
    "    -----------\n",
    "    audio_data (np.ndarray) : audio data from mic streaming (assuming it is 4 channels)\n",
    "    cfg (array)             : array of configuration parameters (from .yml file)\n",
    "    data_config (.yml file) : filepath to the .yml config file used for SALSA-Lite\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    None \n",
    "\n",
    "    To-do\n",
    "    ------\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        # Load data config files\n",
    "        with open(data_config, 'r') as stream:\n",
    "            try:\n",
    "                cfg = yaml.safe_load(stream)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "    # Parse config file\n",
    "    fs = cfg['data']['fs']\n",
    "    n_fft = cfg['data']['n_fft']\n",
    "    hop_length = cfg['data']['hop_len']\n",
    "    win_length = cfg['data']['win_len']\n",
    "\n",
    "    # Doa info\n",
    "    n_mics = 4\n",
    "    fmin_doa = cfg['data']['fmin_doa']\n",
    "    fmax_doa = cfg['data']['fmax_doa']\n",
    "    \n",
    "    \"\"\"\n",
    "    For the demo, fmax_doa = 4kHz, fs = 48kHz, n_fft = 512, hop = 300\n",
    "    This results in the following:\n",
    "        n_bins      = 257\n",
    "        lower_bin   = 1\n",
    "        upper_bin   = 42\n",
    "        cutoff_bin  = 96 \n",
    "        logspecs -> 95 bins total\n",
    "        phasespecs -> 41 bins total\n",
    "        \n",
    "    Since these are all fixed, can we just put them into the config.yml instead\n",
    "    and just read them from there and avoid these calculations\n",
    "    \"\"\"\n",
    "    fmax_doa = np.min((fmax_doa, fs // 2))\n",
    "    n_bins = n_fft // 2 + 1\n",
    "    lower_bin = int(np.floor(fmin_doa * n_fft / float(fs)))\n",
    "    upper_bin = int(np.floor(fmax_doa * n_fft / float(fs)))\n",
    "    lower_bin = np.max((1, lower_bin))\n",
    "\n",
    "    # Cutoff frequency for spectrograms\n",
    "    fmax = 9000  # Hz, meant to reduce feature dimensions\n",
    "    cutoff_bin = int(np.floor(fmax * n_fft / float(fs)))  # 9000 Hz, 512 nfft: cutoff_bin = 192\n",
    "    assert upper_bin <= cutoff_bin, 'Upper bin for spatial feature is higher than cutoff bin for spectrogram!'\n",
    "\n",
    "    # Normalization factor for salsa_lite --> 2*pi*f/c\n",
    "    c = 343\n",
    "    delta = 2 * np.pi * fs / (n_fft * c)\n",
    "    freq_vector = np.arange(n_bins)\n",
    "    freq_vector[0] = 1\n",
    "    freq_vector = freq_vector[:, None, None]  # n_bins x 1 x 1\n",
    "    \n",
    "    # Extract the features from the audio data\n",
    "    \"\"\"\n",
    "    The audio data is from the ambimik, assuming that it is a four-channel array\n",
    "    The shape should be (4 , x) for (n_channels, time*fs)\n",
    "    \"\"\"\n",
    "    log_specs = []\n",
    "    for imic in np.arange(n_mics):\n",
    "        # audio_mic_data = local_scaling(audio_data[imic, :]) \n",
    "        audio_mic_data = audio_data[imic, :]\n",
    "        stft = librosa.stft(y=np.asfortranarray(audio_mic_data), \n",
    "                            n_fft=n_fft, \n",
    "                            hop_length=hop_length,\n",
    "                            center=True, \n",
    "                            window='hann', \n",
    "                            pad_mode='reflect')\n",
    "        if imic == 0:\n",
    "            n_frames = stft.shape[1]\n",
    "            X = np.zeros((n_bins, n_frames, n_mics), dtype='complex')  # (n_bins, n_frames, n_mics)\n",
    "        X[:, :, imic] = stft\n",
    "        # Compute log linear power spectrum\n",
    "        spec = (np.abs(stft) ** 2).T\n",
    "        log_spec = librosa.power_to_db(spec, ref=1.0, amin=1e-10, top_db=None)\n",
    "        log_spec = np.expand_dims(log_spec, axis=0)\n",
    "        log_specs.append(log_spec)\n",
    "    log_specs = np.concatenate(log_specs, axis=0)  # (n_mics, n_frames, n_bins)\n",
    "\n",
    "    # Compute spatial feature\n",
    "    # X : (n_bins, n_frames, n_mics) , NIPD formula : -(c / (2pi x f)) x arg[X1*(t,f) . X2:M(t,f)]\n",
    "    phase_vector = np.angle(X[:, :, 1:] * np.conj(X[:, :, 0, None]))\n",
    "    phase_vector = phase_vector / (delta * freq_vector)\n",
    "    phase_vector = np.transpose(phase_vector, (2, 1, 0))  # (n_mics, n_frames, n_bins)\n",
    "    \n",
    "    # Crop frequency\n",
    "    log_specs = log_specs[:, :, lower_bin:cutoff_bin]\n",
    "    phase_vector = phase_vector[:, :, lower_bin:cutoff_bin]\n",
    "    phase_vector[:, :, upper_bin:] = 0\n",
    "    \n",
    "    # Stack features\n",
    "    audio_feature = np.concatenate((log_specs, phase_vector), axis=0)\n",
    "    audio_feature = audio_feature.astype(np.float32)\n",
    "    \n",
    "    # Now we normalize the first 4 log power spectrogram channels of SALSALITE\n",
    "    n_feature_channels = 4\n",
    "    scaler_dict = {}\n",
    "    for i_chan in np.arange(n_feature_channels):\n",
    "        scaler_dict[i_chan] = preprocessing.StandardScaler()\n",
    "        scaler_dict[i_chan].partial_fit(audio_feature[i_chan, : , : ]) # (n_timesteps, n_features)\n",
    "        \n",
    "    # Extract mean and std\n",
    "    feature_mean = []\n",
    "    feature_std = []\n",
    "    for i_chan in range(n_feature_channels):\n",
    "        feature_mean.append(scaler_dict[i_chan].mean_)\n",
    "        feature_std.append(np.sqrt(scaler_dict[i_chan].var_))\n",
    "\n",
    "    feature_mean = np.array(feature_mean)\n",
    "    feature_std = np.array(feature_std)\n",
    "\n",
    "    # Expand dims for timesteps: (n_chanels, n_timesteps, n_features)\n",
    "    feature_mean = np.expand_dims(feature_mean, axis=1)\n",
    "    feature_std = np.expand_dims(feature_std, axis=1)\n",
    "    audio_feature[:4] = (audio_feature[:4] - feature_mean)/feature_std\n",
    "        \n",
    "    return audio_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_index():\n",
    "    return random.randint(0,4900)\n",
    "\n",
    "def get_random_numframes_replace():\n",
    "    return random.randint(0,7)\n",
    "\n",
    "def get_mask(i):\n",
    "    if i == 0:\n",
    "        return [True, False, False, False, False]\n",
    "    elif i == 1:\n",
    "        return [False, True, True, True, True]\n",
    "    elif i == 2:\n",
    "        return [False, False, True, True, True]\n",
    "    elif i == 3:\n",
    "        return [False, False, False, True, True]\n",
    "    elif i == 4:\n",
    "        return [False, False, False, False, True]\n",
    "    elif i == 5:\n",
    "        return [True, True, True, True, False]\n",
    "    elif i == 6:\n",
    "        return [True, True, True, False, False]\n",
    "    elif i == 7:\n",
    "        return [True, True, False, False, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat all noise data together and form a pool of ambient sound data to populate our active sound data with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 48000\n",
    "concatenated_noise_track_fp = \"./data/Dataset_concatenated_tracks/Noise/noise_scaled_0.wav\"\n",
    "all_noise_data, _ = librosa.load(concatenated_noise_track_fp, sr=fs , mono=False, dtype=np.float32)\n",
    "\n",
    "samples_per_frame = int(fs * 0.1)\n",
    "all_noise_frames = [all_noise_data[: , i:i+samples_per_frame] for i in range(0, len(all_noise_data[0]), samples_per_frame)]\n",
    "all_noise_frames_array = np.array(all_noise_frames)\n",
    "\n",
    "print(all_noise_frames_array.shape)\n",
    "# (how many 100ms frame, 4 channels, sample points for 100ms frame (4800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we augment each segmeneted audio data file at random with noise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data_fp = \"./_audio/cleaned_data_0.5s_0.25s/\"\n",
    "active_classes_dir = ['dog', 'impact', 'speech']\n",
    "active_classes = ['dog', 'impact', 'speech', 'noise']\n",
    "fs = 48000\n",
    "samples_per_frame = int(fs * 0.1)\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for cls in active_classes_dir:\n",
    "    class_filepath = os.path.join(segmented_data_fp, cls)\n",
    "    for file in tqdm(os.listdir(class_filepath)):\n",
    "        if file.endswith('.wav'):\n",
    "            # Get the clean, segmented audio\n",
    "            audio_filepath = os.path.join(class_filepath, file)\n",
    "            audio_data, _ = librosa.load(audio_filepath, sr=fs, mono=False, dtype=np.float32)\n",
    "            \n",
    "            # Here, we get the mask to see which 100ms frames to keep/replace\n",
    "            replacement = get_mask(get_random_numframes_replace())\n",
    "            \n",
    "            # Converting the audio data into 100ms frames\n",
    "            feature_audio_frames = [audio_data[: , i:i+samples_per_frame] for i in range(0, len(audio_data[0]), samples_per_frame)]\n",
    "            \n",
    "            augmented_audio = []\n",
    "            \n",
    "            # Here we either keep the class audio, or we replace them with a random noise audio\n",
    "            for idx, keep in enumerate(replacement):\n",
    "                if keep:\n",
    "                    augmented_audio.append(feature_audio_frames[idx])\n",
    "                else:\n",
    "                    noiseframe_idx = get_random_index()\n",
    "                    augmented_audio.append(all_noise_frames_array[noiseframe_idx])\n",
    "                    \n",
    "            # Combine them all together \n",
    "            augmented_audio_array = np.array(augmented_audio)\n",
    "            augmented_audio_array = np.concatenate(augmented_audio_array, axis = -1)\n",
    "            \n",
    "            # Finally, we convert this augmented audio into features\n",
    "            salsalite_features = extract_features(augmented_audio_array)\n",
    "            \n",
    "            # filename --> class_azimuth_index.wav, get the ground truths\n",
    "            filename_gts = file.replace('.wav' , \"\")\n",
    "            gts = filename_gts.split(\"_\")\n",
    "            \n",
    "            # This is for active class ground truth (sed, doax, doay)\n",
    "            one_frame_gt = np.zeros(len(active_classes) * 3, dtype=np.float32)\n",
    "            class_index = active_classes.index(gts[0])\n",
    "            one_frame_gt[class_index] = 1\n",
    "            \n",
    "            # Converting Azimuth into radians and assigning to active class\n",
    "            gt_azi = int(gts[1])\n",
    "            # Convert the azimuth from [0, 360) to [-180, 180), taking (0 == 0) and (180 == -180)\n",
    "            if gt_azi == 330:\n",
    "                gt_azi = -30\n",
    "            elif gt_azi == 270:\n",
    "                gt_azi = -90\n",
    "            elif gt_azi == 210:\n",
    "                gt_azi = -150\n",
    "\n",
    "            azi_rad = gt_azi * np.pi / 180.0 # Convert to radian unit this way\n",
    "            one_frame_gt[class_index + len(active_classes)] = np.cos(azi_rad) # X-coordinate\n",
    "            one_frame_gt[class_index + 2 * len(active_classes)] = np.sin(azi_rad) # Y-coordinate\n",
    "            # Produce the ground truth labels for a single frame, expand the frame dimensions later\n",
    "            one_frame_gt = np.reshape(one_frame_gt, (1, len(one_frame_gt)))\n",
    "            \n",
    "            # This is for noise frame ground truth\n",
    "            noise_frame_gt = np.zeros(len(active_classes) * 3, dtype=np.float32)\n",
    "            noise_frame_gt = np.reshape(noise_frame_gt, (1, len(noise_frame_gt)))\n",
    "            \n",
    "            # Finally, we combine the active class and noise ground truths \n",
    "            gt_result = np.empty((5, len(active_classes)*3))\n",
    "            for is_kept in replacement:\n",
    "                if is_kept:\n",
    "                    gt_result = np.concatenate((gt_result, one_frame_gt), axis=0)\n",
    "                else:\n",
    "                    gt_result = np.concatenate((gt_result, noise_frame_gt), axis=0)\n",
    "            \n",
    "            all_features.append(salsalite_features)\n",
    "            all_labels.append(gt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_storagepath = \"./training_datasets/demo_dataset_0.5s_0.25s_wgn20\"\n",
    "feature_fp = os.path.join(final_dataset_storagepath, \"augmented_salsalite_features.npy\")\n",
    "np.save(feature_fp, all_features, allow_pickle=True)\n",
    "\n",
    "label_fp = os.path.join(final_dataset_storagepath, \"augmented_salsalite_labels.npy\")\n",
    "np.save(label_fp, all_labels, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-lite-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
